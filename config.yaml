model:
  tokenizer_training_size: 0.25
  train_test_ratio: 0.8
  vocab_size: 300
  block_size: 128 # T (context window)
  batch_size: 32 # B (data points per batch)
  d_model: 64 # C (embeddings dimension)
  num_heads: 2
  num_transformer_blocks: 4
  dropout_rate: 0.1
  device: "cpu"
