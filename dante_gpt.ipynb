{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCNV07hhci/rCvBoaDGSIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nMaax/danteGPT/blob/main/dante_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and tokenization"
      ],
      "metadata": {
        "id": "8Z5ZMZrHbvTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nMaax/danteGPT/main/divina_commedia.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXyoc5dtwdQM",
        "outputId": "62f7b455-4660-4726-f58b-9744b3dbc37f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-19 22:10:34--  https://raw.githubusercontent.com/nMaax/danteGPT/main/divina_commedia.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 535249 (523K) [text/plain]\n",
            "Saving to: ‘divina_commedia.txt’\n",
            "\n",
            "divina_commedia.txt 100%[===================>] 522.70K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-19 22:10:34 (32.8 MB/s) - ‘divina_commedia.txt’ saved [535249/535249]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file\n",
        "with open('divina_commedia.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "kSRe8AC0s55I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:10000])"
      ],
      "metadata": {
        "id": "uNnBfFh9s-ML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf969ac7-3223-424f-80ed-39656f8565b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFERNO CANTO 1\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura\n",
            "ché la diritta via era smarrita.\n",
            "Ahi quanto a dir qual era è cosa dura\n",
            "esta selva selvaggia e aspra e forte\n",
            "che nel pensier rinova la paura!\n",
            "Tant' è amara che poco è più morte;\n",
            "ma per trattar del ben ch'i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v'ho scorte.\n",
            "Io non so ben ridir com' i' v'intrai,\n",
            "tant' era pien di sonno a quel punto\n",
            "che la verace via abbandonai.\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava quella valle\n",
            "che m'avea di paura il cor compunto,\n",
            "guardai in alto, e vidi le sue spalle\n",
            "vestite già de' raggi del pianeta\n",
            "che mena dritto altrui per ogne calle.\n",
            "Allor fu la paura un poco queta\n",
            "che nel lago del cor m'era durata\n",
            "la notte ch'i' passai con tanta pieta.\n",
            "E come quei che con lena affannata\n",
            "uscito fuor del pelago a la riva\n",
            "si volge a l'acqua perigliosa e guata,\n",
            "così l'animo mio, ch'ancor fuggiva,\n",
            "si volse a retro a rimirar lo passo\n",
            "che non lasciò già mai persona viva.\n",
            "Poi ch'èi posato un poco il corpo lasso,\n",
            "ripresi via per la piaggia diserta,\n",
            "sì che 'l piè fermo sempre era 'l più basso.\n",
            "Ed ecco, quasi al cominciar de l'erta,\n",
            "una lonza leggera e presta molto,\n",
            "che di pel macolato era coverta;\n",
            "e non mi si partia dinanzi al volto,\n",
            "anzi 'mpediva tanto il mio cammino,\n",
            "ch'i' fui per ritornar più volte vòlto.\n",
            "Temp' era dal principio del mattino,\n",
            "e 'l sol montava 'n sù con quelle stelle\n",
            "ch'eran con lui quando l'amor divino\n",
            "mosse di prima quelle cose belle;\n",
            "sì ch'a bene sperar m'era cagione\n",
            "di quella fiera a la gaetta pelle\n",
            "l'ora del tempo e la dolce stagione;\n",
            "ma non sì che paura non mi desse\n",
            "la vista che m'apparve d'un leone.\n",
            "Questi parea che contra me venisse\n",
            "con la test' alta e con rabbiosa fame,\n",
            "sì che parea che l'aere ne tremesse.\n",
            "Ed una lupa, che di tutte brame\n",
            "sembiava carca ne la sua magrezza,\n",
            "e molte genti fé già viver grame,\n",
            "questa mi porse tanto di gravezza\n",
            "con la paura ch'uscia di sua vista,\n",
            "ch'io perdei la speranza de l'altezza.\n",
            "E qual è quei che volontieri acquista,\n",
            "e giugne 'l tempo che perder lo face,\n",
            "che 'n tutti suoi pensier piange e s'attrista;\n",
            "tal mi fece la bestia sanza pace,\n",
            "che, venendomi 'ncontro, a poco a poco\n",
            "mi ripigneva là dove 'l sol tace.\n",
            "Mentre ch'i' rovinava in basso loco,\n",
            "dinanzi a li occhi mi si fu offerto\n",
            "chi per lungo silenzio parea fioco.\n",
            "Quando vidi costui nel gran diserto,\n",
            "<<\n",
            "Miserere\n",
            "di me>>, gridai a lui,\n",
            "<<qual che tu sii, od ombra od omo certo!>>.\n",
            "Rispuosemi: <<Non omo, omo già fui,\n",
            "e li parenti miei furon lombardi,\n",
            "mantoani per patria ambedui.\n",
            "Nacqui\n",
            "sub Iulio\n",
            ", ancor che fosse tardi,\n",
            "e vissi a Roma sotto 'l buono Augusto\n",
            "nel tempo de li dèi falsi e bugiardi.\n",
            "Poeta fui, e cantai di quel giusto\n",
            "figliuol d'Anchise che venne di Troia,\n",
            "poi che 'l superbo Ilión fu combusto.\n",
            "Ma tu perché ritorni a tanta noia?\n",
            "perché non sali il dilettoso monte\n",
            "ch'è principio e cagion di tutta gioia?>>.\n",
            "<<Or se' tu quel Virgilio e quella fonte\n",
            "che spandi di parlar sì largo fiume?>>,\n",
            "rispuos' io lui con vergognosa fronte.\n",
            "<<O de li altri poeti onore e lume\n",
            "vagliami 'l lungo studio e 'l grande amore\n",
            "che m'ha fatto cercar lo tuo volume.\n",
            "Tu se' lo mio maestro e 'l mio autore;\n",
            "tu se' solo colui da cu' io tolsi\n",
            "lo bello stilo che m'ha fatto onore.\n",
            "Vedi la bestia per cu' io mi volsi:\n",
            "aiutami da lei, famoso saggio,\n",
            "ch'ella mi fa tremar le vene e i polsi>>.\n",
            "<<A te convien tenere altro viaggio>>,\n",
            "rispuose poi che lagrimar mi vide,\n",
            "<<se vuo' campar d'esto loco selvaggio;\n",
            "ché questa bestia, per la qual tu gride,\n",
            "non lascia altrui passar per la sua via,\n",
            "ma tanto lo 'mpedisce che l'uccide;\n",
            "e ha natura sì malvagia e ria,\n",
            "che mai non empie la bramosa voglia,\n",
            "e dopo 'l pasto ha più fame che pria.\n",
            "Molti son li animali a cui s'ammoglia,\n",
            "e più saranno ancora, infin che 'l veltro\n",
            "verrà, che la farà morir con doglia.\n",
            "Questi non ciberà terra né peltro,\n",
            "ma sapienza, amore e virtute,\n",
            "e sua nazion sarà tra feltro e feltro.\n",
            "Di quella umile Italia fia salute\n",
            "per cui morì la vergine Cammilla,\n",
            "Eurialo e Turno e Niso di ferute.\n",
            "Questi la caccerà per ogne villa,\n",
            "fin che l'avrà rimessa ne lo 'nferno,\n",
            "là onde 'nvidia prima dipartilla.\n",
            "Ond' io per lo tuo me' penso e discerno\n",
            "che tu mi segui, e io sarò tua guida,\n",
            "e trarrotti di qui per loco etterno,\n",
            "ove udirai le disperate strida,\n",
            "vedrai li antichi spiriti dolenti,\n",
            "ch'a la seconda morte ciascun grida;\n",
            "e vederai color che son contenti\n",
            "nel foco, perché speran di venire\n",
            "quando che sia a le beate genti.\n",
            "A le quai poi se tu vorrai salire,\n",
            "anima fia a ciò più di me degna:\n",
            "con lei ti lascerò nel mio partire;\n",
            "ché quello imperador che là sù regna,\n",
            "perch' i' fu' ribellante a la sua legge,\n",
            "non vuol che 'n sua città per me si vegna.\n",
            "In tutte parti impera e quivi regge;\n",
            "quivi è la sua città e l'alto seggio:\n",
            "oh felice colui cu' ivi elegge!>>.\n",
            "E io a lui: <<Poeta, io ti richeggio\n",
            "per quello Dio che tu non conoscesti,\n",
            "acciò ch'io fugga questo male e peggio,\n",
            "che tu mi meni là dov'or dicesti,\n",
            "sì ch'io veggia la porta di san Pietro\n",
            "e color cui tu fai cotanto mesti>>.\n",
            "Allor si mosse, e io li tenni dietro.\n",
            "\n",
            "INFERNO CANTO 2\n",
            "Lo giorno se n'andava, e l'aere bruno\n",
            "toglieva li animai che sono in terra\n",
            "da le fatiche loro; e io sol uno\n",
            "m'apparecchiava a sostener la guerra\n",
            "sì del cammino e sì de la pietate,\n",
            "che ritrarrà la mente che non erra.\n",
            "O muse, o alto ingegno, or m'aiutate;\n",
            "o mente che scrivesti ciò ch'io vidi,\n",
            "qui si parrà la tua nobilitate.\n",
            "Io cominciai: <<Poeta che mi guidi,\n",
            "guarda la mia virtù s'ell' è possente,\n",
            "prima ch'a l'alto passo tu mi fidi.\n",
            "Tu dici che di Silvio il parente,\n",
            "corruttibile ancora, ad immortale\n",
            "secolo andò, e fu sensibilmente.\n",
            "Però, se l'avversario d'ogne male\n",
            "cortese i fu, pensando l'alto effetto\n",
            "ch'uscir dovea di lui e 'l chi e 'l quale\n",
            "non pare indegno ad omo d'intelletto;\n",
            "ch'e' fu de l'alma Roma e di suo impero\n",
            "ne l'empireo ciel per padre eletto:\n",
            "la quale e 'l quale, a voler dir lo vero,\n",
            "fu stabilita per lo loco santo\n",
            "u' siede il successor del maggior Piero.\n",
            "Per quest' andata onde li dai tu vanto,\n",
            "intese cose che furon cagione\n",
            "di sua vittoria e del papale ammanto.\n",
            "Andovvi poi lo Vas d'elezione,\n",
            "per recarne conforto a quella fede\n",
            "ch'è principio a la via di salvazione.\n",
            "Ma io, perché venirvi? o chi 'l concede?\n",
            "Io non Enea, io non Paulo sono;\n",
            "me degno a ciò né io né altri 'l crede.\n",
            "Per che, se del venire io m'abbandono,\n",
            "temo che la venuta non sia folle.\n",
            "Se' savio; intendi me' ch'i' non ragiono>>.\n",
            "E qual è quei che disvuol ciò che volle\n",
            "e per novi pensier cangia proposta,\n",
            "sì che dal cominciar tutto si tolle,\n",
            "tal mi fec' io 'n quella oscura costa,\n",
            "perché, pensando, consumai la 'mpresa\n",
            "che fu nel cominciar cotanto tosta.\n",
            "<<S'i' ho ben la parola tua intesa>>,\n",
            "rispuose del magnanimo quell' ombra;\n",
            "<<l'anima tua è da viltade offesa;\n",
            "la qual molte fiate l' omo ingombra\n",
            "sì che d'onrata impresa lo rivolve,\n",
            "come falso veder bestia quand' ombra.\n",
            "Da questa tema acciò che tu ti solve,\n",
            "dirotti perch' io venni e quel ch'io 'ntesi\n",
            "nel primo punto che di te mi dolve.\n",
            "Io era tra color che son sospesi,\n",
            "e donna mi chiamò beata e bella,\n",
            "tal che di comandare io la richiesi.\n",
            "Lucevan li occhi suoi più che la stella;\n",
            "e cominciommi a dir soave e piana,\n",
            "con angelica voce, in sua favella:\n",
            "\"O anima cortese mantoana,\n",
            "di cui la fama ancor nel mondo dura,\n",
            "e durerà quanto 'l mondo lontana,\n",
            "l'amico mio, e non de la ventura,\n",
            "ne la diserta piaggia è impedito\n",
            "sì nel cammin, che volt' è per paura;\n",
            "e temo che non sia già sì smarrito,\n",
            "ch'io mi sia tardi al soccorso levata,\n",
            "per quel ch'i' ho di lui nel cielo udito.\n",
            "Or movi, e con la tua parola ornata\n",
            "e con ciò c'ha mestieri al suo campare,\n",
            "l'aiuta, sì ch'i' ne sia consolata.\n",
            "I' son Beatrice che ti faccio andare;\n",
            "vegno del loco ove tornar disio;\n",
            "amor mi mosse, che mi fa parlare.\n",
            "Quando sarò dinanzi al segnor mio,\n",
            "di te mi loderò sovente a lui\".\n",
            "Tacette allora, e poi comincia' io:\n",
            "\"O donna di virtù, sola per cui\n",
            "l'umana spezie eccede ogne contento\n",
            "di quel ciel c'ha minor li cerchi sui,\n",
            "tanto m'aggrada il tuo comandamento,\n",
            "che l'ubidir, se già fosse, m'è tardi;\n",
            "più non t'è uo' ch'aprirmi il tuo talento.\n",
            "Ma dimmi la cagion che non ti guardi\n",
            "de lo scender qua giuso in questo centro\n",
            "de l'ampio loco ove tornar tu ardi\".\n",
            "\"Da che tu vuo' saver cotanto a dentro,\n",
            "dirotti brievemente\", mi rispuose,\n",
            "\"perch' i' non temo di venir qua entro.\n",
            "Temer si dee di sole quelle cose\n",
            "c'hanno potenza di fare altrui male;\n",
            "de l'altre no, ché non son paurose.\n",
            "I' son fatta da Dio, sua mercé, tale,\n",
            "che la vostra miseria non mi tange,\n",
            "né fiamma d'esto 'ncendio non m'assale.\n",
            "Donna è gentil nel ciel che si compiange\n",
            "di questo 'mpedimento ov' io ti mando,\n",
            "sì che duro giudicio là sù frange.\n",
            "Questa chiese Lucia in suo dimando\n",
            "e disse: – Or ha bisogno il tuo fedele\n",
            "di te, e io a te lo raccomando -.\n",
            "Lucia, nimica di ciascun crudele,\n",
            "si mosse, e venne al loco dov' i' era,\n",
            "che mi sedea con l'antica Rachele.\n",
            "Disse: – Beatrice, loda di Dio vera,\n",
            "ché non soccorri quei che t'amò tanto,\n",
            "ch'uscì per te de la volgare schiera?\n",
            "Non odi tu la pieta del suo pianto,\n",
            "non vedi tu la morte che 'l combatte\n",
            "su la fiumana ove 'l mar non ha vanto? -.\n",
            "Al mondo non fur mai persone ratte\n",
            "a far lor pro o a fuggir lor danno,\n",
            "com' io, dopo cotai parole fatte,\n",
            "venni qua giù del mio beato scanno,\n",
            "fidandomi del tuo parlare onesto,\n",
            "ch'onora te e quei ch'udito l'hanno\".\n",
            "Poscia che m'ebbe ragionato questo,\n",
            "li occhi lucenti lacrimando volse,\n",
            "per che mi fece del venir più presto.\n",
            "E venni a te così com' ella volse;\n",
            "d'inanzi a quella fiera ti levai\n",
            "che del bel monte il corto andar ti tolse.\n",
            "Dunque: che è? perché, perché restai,\n",
            "perché tanta viltà nel core allette,\n",
            "perché ardire e franchezza non hai,\n",
            "poscia che tai tre donne benedette\n",
            "curan di te ne la corte del cielo,\n",
            "e 'l mio parlar tanto ben ti promette?>>.\n",
            "Quali fioretti dal notturno gelo\n",
            "chinati e chiusi, poi che 'l sol li 'mbianca\n",
            "si drizzan tutti aperti in loro stelo,\n",
            "tal mi fec' io di mia virtude stanca,\n",
            "e tanto buono ardire al cor mi corse,\n",
            "ch'i' cominciai come persona franca:\n",
            "<<Oh pietosa colei che mi soccorse!\n",
            "e te cortese ch'ubidisti tosto\n",
            "a le vere parole che ti porse!\n",
            "Tu m'hai co\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import regex as re\n",
        "\n",
        "class BasicTokenizer():\n",
        "  def __init__(self,):\n",
        "    self.merges = None\n",
        "    self.rev_merges = None\n",
        "    self.vocab = None\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "\n",
        "    num_to_merge = vocab_size - 256\n",
        "    tokens = list(map(int, text.encode('utf-8')))\n",
        "    _, self.merges, self.rev_merges = self.continuous_merge(tokens, num_to_merge)\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\"\n",
        "    Encodes text into merged tokens using a given merge map.\n",
        "\n",
        "    - Converts text to UTF-8 byte tokens.\n",
        "    - Iteratively applies merges, replacing bigrams with new indices.\n",
        "    \"\"\"\n",
        "    merges=self.merges\n",
        "    tokens = list(text.encode('utf-8'))  # Get byte representation\n",
        "    for bigram, new_idx in merges.items():\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == bigram:\n",
        "                new_tokens.append(new_idx)  # Merge the bigram\n",
        "                i += 2  # Skip next token (merged)\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        tokens = new_tokens  # Update tokens after each merge pass\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    \"\"\"Decodes merged tokens back into a UTF-8 string.\"\"\"\n",
        "\n",
        "    vocab = {token: bytes([token]) for token in range(256)}\n",
        "    for (original_token_1, original_token_2), merged_token in self.merges.items():\n",
        "        vocab[merged_token] = vocab[original_token_1] + vocab[original_token_2]\n",
        "    self.vocab = vocab\n",
        "\n",
        "    tokens = b\"\".join(vocab[token] for token in tokens)\n",
        "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text\n",
        "\n",
        "  def count_bigrams(self, tokens):\n",
        "    \"\"\"Count bigrams using collections.Counter for efficiency.\"\"\"\n",
        "    return Counter(zip(tokens[:-1], tokens[1:]))\n",
        "\n",
        "  def sort_counter(self, counter, reverse=True):\n",
        "      \"\"\"Sort bigrams by frequency.\n",
        "\n",
        "      Returns a list of (count, bigram) tuples.\n",
        "      \"\"\"\n",
        "      return sorted(((count, bigram) for bigram, count in counter.items()), reverse=reverse)\n",
        "\n",
        "  def merge(self, tokens, bigram, new_idx):\n",
        "      \"\"\"\n",
        "      Merge occurrences of `bigram` in the token list.\n",
        "\n",
        "      Instead of modifying the list in place, build a new list.\n",
        "      When a bigram is found, it is replaced by new_idx, and the second token is skipped.\n",
        "      \"\"\"\n",
        "      new_tokens = []\n",
        "      i = 0\n",
        "      while i < len(tokens):\n",
        "          if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == bigram:\n",
        "              new_tokens.append(new_idx)\n",
        "              i += 2  # Skip the next token as it's merged\n",
        "          else:\n",
        "              new_tokens.append(tokens[i])\n",
        "              i += 1\n",
        "      return new_tokens\n",
        "\n",
        "  def continuous_merge(self, tokens, n_to_merge):\n",
        "      \"\"\"\n",
        "      Merge tokens continuously for n_to_merge iterations.\n",
        "\n",
        "      Operates on a copy of the tokens so that the original list is not modified.\n",
        "      Returns the merged tokens and a mapping of the merged bigrams to new indices.\n",
        "      \"\"\"\n",
        "      tokens = tokens.copy()  # Work on a copy to leave the original unchanged.\n",
        "      # Compute a base index that is outside the valid UTF-8 range.\n",
        "      base = 256\n",
        "      merges = {}\n",
        "      rev_merges = {}\n",
        "\n",
        "      for merge_iteration in range(n_to_merge):\n",
        "          counter = self.count_bigrams(tokens)\n",
        "          if not counter:\n",
        "              break\n",
        "          sorted_counter = self.sort_counter(counter, reverse=True)\n",
        "          # Get the most frequent bigram\n",
        "          bigram_to_merge = sorted_counter[0][1]\n",
        "          new_idx = base + merge_iteration\n",
        "          tokens = self.merge(tokens, bigram_to_merge, new_idx)\n",
        "          merges[bigram_to_merge] = new_idx\n",
        "          rev_merges[new_idx] = bigram_to_merge\n",
        "      return tokens, merges, rev_merges\n",
        "\n",
        "class RegexTokenizer(BasicTokenizer):\n",
        "  def __init__(self, pattern=None):\n",
        "    super().__init__()\n",
        "    # TODO: Improve specifically for Dante\n",
        "    self.pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\" if not pattern else pattern\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "    \"\"\"\n",
        "    Trains the tokenizer by performing byte-pair encoding on the tokenized text.\n",
        "    - Uses regex-based initial tokenization.\n",
        "    - Learns merges similar to BasicTokenizer.\n",
        "    \"\"\"\n",
        "    num_to_merge = vocab_size - 256\n",
        "\n",
        "    # split the text up into text chunks\n",
        "    text_chunks = re.findall(self.pattern, text)\n",
        "\n",
        "    # input text preprocessing\n",
        "    tokenized_text = [list(chunk.encode(\"utf-8\")) for chunk in text_chunks]\n",
        "    _, self.merges, self.rev_merges = self.continuous_merge(tokenized_text, num_to_merge)\n",
        "\n",
        "  def count_bigrams(self, tokenized_text):\n",
        "    return Counter(bigram for word in tokenized_text for bigram in zip(word[:-1], word[1:]))\n",
        "\n",
        "  def continuous_merge(self, tokenized_text, n_to_merge):\n",
        "      \"\"\"\n",
        "      Merge tokens continuously for n_to_merge iterations.\n",
        "\n",
        "      Operates on a copy of the tokens so that the original list is not modified.\n",
        "      Returns the merged tokens and a mapping of the merged bigrams to new indices.\n",
        "      \"\"\"\n",
        "      tokenized_text = tokenized_text.copy()  # Work on a copy to leave the original unchanged.\n",
        "\n",
        "      # Compute a base index that is outside the valid UTF-8 range.\n",
        "      base = 256\n",
        "      merges = {}\n",
        "      rev_merges = {}\n",
        "\n",
        "      for merge_iteration in range(n_to_merge):\n",
        "          counter = self.count_bigrams(tokenized_text=tokenized_text)\n",
        "          if not counter:\n",
        "              break\n",
        "          sorted_counter = self.sort_counter(counter, reverse=True)\n",
        "          # Get the most frequent bigram\n",
        "          bigram_to_merge, count = max(counter.items(), key=lambda item: item[1])\n",
        "          new_idx = base + merge_iteration\n",
        "          tokenized_text = [self.merge(tokenized_word, bigram_to_merge, new_idx) for tokenized_word in tokenized_text]\n",
        "          merges[bigram_to_merge] = new_idx\n",
        "          rev_merges[new_idx] = bigram_to_merge\n",
        "      return tokenized_text, merges, rev_merges"
      ],
      "metadata": {
        "id": "7J6jMDjC9-cx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_training_size = len(text) // 4\n",
        "vocab_size = 500\n",
        "\n",
        "Dantokenizer = RegexTokenizer()\n",
        "Dantokenizer.train(text[:tokenizer_training_size], vocab_size=vocab_size)"
      ],
      "metadata": {
        "id": "NWvA8bLa-LU7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = Dantokenizer.encode\n",
        "decode = Dantokenizer.decode"
      ],
      "metadata": {
        "id": "CslBsRgs-BlX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode('Nel mezzo del cammin di nostra vita, mi ritrovai in una selva oscura.')))"
      ],
      "metadata": {
        "id": "I0GmeEENuyhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead1e315-73fa-42b3-97eb-eece5f6b3088"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nel mezzo del cammin di nostra vita, mi ritrovai in una selva oscura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline, Transformer-free model"
      ],
      "metadata": {
        "id": "-UiF4zFRbpH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "edAktZYGw1NX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "66LD44jpzyVB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgE06JSTz4Sq",
        "outputId": "7f05e3cc-0d2a-4261-980e-21d80f611a35"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([259412]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ],
      "metadata": {
        "id": "b9jaBdnk0Eiy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print(\"full tokenized sentence: \", data[:block_size+1])\n",
        "print()\n",
        "for t in range(block_size):\n",
        "  print(\"context:\", x[:t+1])\n",
        "  print(\"target: \", y[t].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEXMKf4V0R67",
        "outputId": "9fba9d33-ac7c-498f-ec28-316c61ea3cf3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full tokenized sentence:  tensor([ 73,  78,  70,  69,  82,  78,  79, 445,  65])\n",
            "\n",
            "context: tensor([73])\n",
            "target:  78\n",
            "context: tensor([73, 78])\n",
            "target:  70\n",
            "context: tensor([73, 78, 70])\n",
            "target:  69\n",
            "context: tensor([73, 78, 70, 69])\n",
            "target:  82\n",
            "context: tensor([73, 78, 70, 69, 82])\n",
            "target:  78\n",
            "context: tensor([73, 78, 70, 69, 82, 78])\n",
            "target:  79\n",
            "context: tensor([73, 78, 70, 69, 82, 78, 79])\n",
            "target:  445\n",
            "context: tensor([ 73,  78,  70,  69,  82,  78,  79, 445])\n",
            "target:  65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "def get_batch(split='train', batch_size=32, block_size=8):\n",
        "  data = train_data if split=='train' else test_data\n",
        "  ix = torch.randint(low=0, high=(len(data) - block_size), size=(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  y = torch.clamp(y, 0, vocab_size - 1)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "q2W8GtBu0vOM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train', batch_size=4, block_size=8)"
      ],
      "metadata": {
        "id": "0j7p9W3I1hK_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb.shape, xb, sep='\\n')\n",
        "print(yb.shape, yb, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGdxIBNL13Ni",
        "outputId": "fbb68c6f-906d-4454-d4d7-520e7bc0af10"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8])\n",
            "tensor([[363, 328, 317, 348, 475, 266, 318, 321],\n",
            "        [266,  97, 256, 330, 384, 362, 276, 388],\n",
            "        [264, 293, 260, 297, 281, 353, 279, 261],\n",
            "        [266, 438, 392,  97, 273, 111, 418, 280]])\n",
            "torch.Size([4, 8])\n",
            "tensor([[328, 317, 348, 475, 266, 318, 321, 441],\n",
            "        [ 97, 256, 330, 384, 362, 276, 388, 277],\n",
            "        [293, 260, 297, 281, 353, 279, 261, 361],\n",
            "        [438, 392,  97, 273, 111, 418, 280, 265]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "for b in range(8):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when the context is {context} then the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMXFd0xY3GG3",
        "outputId": "3f93a0f3-93ee-4d01-8e30-3c825aaeff1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when the context is tensor([268]) then the target is 271\n",
            "when the context is tensor([268, 271]) then the target is 65\n",
            "when the context is tensor([268, 271,  65]) then the target is 392\n",
            "when the context is tensor([268, 271,  65, 392]) then the target is 97\n",
            "when the context is tensor([268, 271,  65, 392,  97]) then the target is 455\n",
            "when the context is tensor([268, 271,  65, 392,  97, 455]) then the target is 339\n",
            "when the context is tensor([268, 271,  65, 392,  97, 455, 339]) then the target is 32\n",
            "when the context is tensor([268, 271,  65, 392,  97, 455, 339,  32]) then the target is 104\n",
            "when the context is tensor([333]) then the target is 101\n",
            "when the context is tensor([333, 101]) then the target is 260\n",
            "when the context is tensor([333, 101, 260]) then the target is 101\n",
            "when the context is tensor([333, 101, 260, 101]) then the target is 425\n",
            "when the context is tensor([333, 101, 260, 101, 425]) then the target is 101\n",
            "when the context is tensor([333, 101, 260, 101, 425, 101]) then the target is 32\n",
            "when the context is tensor([333, 101, 260, 101, 425, 101,  32]) then the target is 304\n",
            "when the context is tensor([333, 101, 260, 101, 425, 101,  32, 304]) then the target is 349\n",
            "when the context is tensor([354]) then the target is 33\n",
            "when the context is tensor([354,  33]) then the target is 10\n",
            "when the context is tensor([354,  33,  10]) then the target is 83\n",
            "when the context is tensor([354,  33,  10,  83]) then the target is 39\n",
            "when the context is tensor([354,  33,  10,  83,  39]) then the target is 287\n",
            "when the context is tensor([354,  33,  10,  83,  39, 287]) then the target is 278\n",
            "when the context is tensor([354,  33,  10,  83,  39, 287, 278]) then the target is 320\n",
            "when the context is tensor([354,  33,  10,  83,  39, 287, 278, 320]) then the target is 301\n",
            "when the context is tensor([266]) then the target is 318\n",
            "when the context is tensor([266, 318]) then the target is 301\n",
            "when the context is tensor([266, 318, 301]) then the target is 110\n",
            "when the context is tensor([266, 318, 301, 110]) then the target is 257\n",
            "when the context is tensor([266, 318, 301, 110, 257]) then the target is 195\n",
            "when the context is tensor([266, 318, 301, 110, 257, 195]) then the target is 185\n",
            "when the context is tensor([266, 318, 301, 110, 257, 195, 185]) then the target is 332\n",
            "when the context is tensor([266, 318, 301, 110, 257, 195, 185, 332]) then the target is 257\n",
            "when the context is tensor([267]) then the target is 378\n",
            "when the context is tensor([267, 378]) then the target is 291\n",
            "when the context is tensor([267, 378, 291]) then the target is 103\n",
            "when the context is tensor([267, 378, 291, 103]) then the target is 292\n",
            "when the context is tensor([267, 378, 291, 103, 292]) then the target is 400\n",
            "when the context is tensor([267, 378, 291, 103, 292, 400]) then the target is 275\n",
            "when the context is tensor([267, 378, 291, 103, 292, 400, 275]) then the target is 259\n",
            "when the context is tensor([267, 378, 291, 103, 292, 400, 275, 259]) then the target is 334\n",
            "when the context is tensor([97]) then the target is 314\n",
            "when the context is tensor([ 97, 314]) then the target is 424\n",
            "when the context is tensor([ 97, 314, 424]) then the target is 324\n",
            "when the context is tensor([ 97, 314, 424, 324]) then the target is 265\n",
            "when the context is tensor([ 97, 314, 424, 324, 265]) then the target is 257\n",
            "when the context is tensor([ 97, 314, 424, 324, 265, 257]) then the target is 346\n",
            "when the context is tensor([ 97, 314, 424, 324, 265, 257, 346]) then the target is 487\n",
            "when the context is tensor([ 97, 314, 424, 324, 265, 257, 346, 487]) then the target is 302\n",
            "when the context is tensor([116]) then the target is 402\n",
            "when the context is tensor([116, 402]) then the target is 115\n",
            "when the context is tensor([116, 402, 115]) then the target is 256\n",
            "when the context is tensor([116, 402, 115, 256]) then the target is 108\n",
            "when the context is tensor([116, 402, 115, 256, 108]) then the target is 277\n",
            "when the context is tensor([116, 402, 115, 256, 108, 277]) then the target is 105\n",
            "when the context is tensor([116, 402, 115, 256, 108, 277, 105]) then the target is 283\n",
            "when the context is tensor([116, 402, 115, 256, 108, 277, 105, 283]) then the target is 281\n",
            "when the context is tensor([118]) then the target is 277\n",
            "when the context is tensor([118, 277]) then the target is 282\n",
            "when the context is tensor([118, 277, 282]) then the target is 279\n",
            "when the context is tensor([118, 277, 282, 279]) then the target is 97\n",
            "when the context is tensor([118, 277, 282, 279,  97]) then the target is 268\n",
            "when the context is tensor([118, 277, 282, 279,  97, 268]) then the target is 271\n",
            "when the context is tensor([118, 277, 282, 279,  97, 268, 271]) then the target is 77\n",
            "when the context is tensor([118, 277, 282, 279,  97, 268, 271,  77]) then the target is 280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class naiveLM(nn.Module):\n",
        "  def __init__(self, vocab_size, latent_space_dim):\n",
        "    super().__init__()\n",
        "    self.embedding_table = nn.Embedding(vocab_size, latent_space_dim)\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(in_features=latent_space_dim, out_features=latent_space_dim, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=latent_space_dim, out_features=latent_space_dim, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=latent_space_dim, out_features=latent_space_dim, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=latent_space_dim, out_features=vocab_size, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, idx):\n",
        "    emb = self.embedding_table(idx)\n",
        "    logits = self.net(emb)\n",
        "    return logits\n",
        "\n",
        "  def compute_loss(self, idx, targets):\n",
        "    logits = self.forward(idx)\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B*T, C)\n",
        "    targets = targets.view(B*T)\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "    return loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens=32):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.forward(idx) # B, T, C\n",
        "      logits = logits[:, -1, :] # B, C\n",
        "      probs = F.softmax(logits, dim=-1) # B, C\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # idx: (B, T) --> (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "zklkoveE3pOU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = naiveLM(vocab_size=vocab_size, latent_space_dim=32)"
      ],
      "metadata": {
        "id": "0cXG8nC65ETL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb.shape, xb, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4TM9dYl_L1t",
        "outputId": "cdff0078-94fe-4d6a-e052-d927cc40307c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8])\n",
            "tensor([[268, 271,  65, 392,  97, 455, 339,  32],\n",
            "        [333, 101, 260, 101, 425, 101,  32, 304],\n",
            "        [354,  33,  10,  83,  39, 287, 278, 320],\n",
            "        [266, 318, 301, 110, 257, 195, 185, 332],\n",
            "        [267, 378, 291, 103, 292, 400, 275, 259],\n",
            "        [ 97, 314, 424, 324, 265, 257, 346, 487],\n",
            "        [116, 402, 115, 256, 108, 277, 105, 283],\n",
            "        [118, 277, 282, 279,  97, 268, 271,  77],\n",
            "        [ 32, 304, 349, 271, 488, 310, 438, 395],\n",
            "        [260, 101, 425, 101, 375, 278, 315, 280],\n",
            "        [315, 335,  97, 314, 347, 490, 260, 274],\n",
            "        [302, 310, 319, 403, 281, 275, 310, 367],\n",
            "        [297, 102, 262, 345,  10, 354, 434, 264],\n",
            "        [342, 468, 260, 299, 122, 335, 105, 271],\n",
            "        [ 32,  83, 448, 327,  97,  10, 318, 338],\n",
            "        [ 99, 335, 111,  10, 109, 298, 479, 277],\n",
            "        [315, 117, 262,  32,  86, 105, 114, 468],\n",
            "        [105,  10, 100, 288, 272, 292, 299, 102],\n",
            "        [300, 116, 277, 262, 493, 455, 391, 265],\n",
            "        [325,  79, 296, 383, 332,  99, 117, 282],\n",
            "        [282, 375, 316, 356, 404, 401, 262, 444],\n",
            "        [ 76, 365, 301, 348, 301, 108, 273, 111],\n",
            "        [100, 304, 316,  99, 299, 443, 103, 368],\n",
            "        [ 10, 407, 304, 366, 280, 118, 263, 278],\n",
            "        [291, 460, 103, 276, 101, 445,  97, 109],\n",
            "        [347, 388, 314, 257, 195, 185, 386, 330],\n",
            "        [298,  99, 261, 101, 316,  99, 299, 105],\n",
            "        [323,  79, 321, 429, 108, 267, 100, 274],\n",
            "        [303, 323, 307,  78, 326, 408, 366, 280],\n",
            "        [263, 423, 101, 332, 295, 115, 295,  44],\n",
            "        [296, 402, 313, 471, 117, 303,  10, 276],\n",
            "        [291, 434, 260, 114, 300, 101, 266, 352]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model.forward(xb)"
      ],
      "metadata": {
        "id": "Q5yNcWIt6w8k"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits.shape, logits, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMGujrDb_Q42",
        "outputId": "68c2806d-a4b8-4b36-8b59-65baa338a4ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8, 500])\n",
            "tensor([[[-0.0973,  0.0823,  0.1829,  ..., -0.0455,  0.0730, -0.0730],\n",
            "         [-0.1163,  0.0633,  0.1870,  ..., -0.0152,  0.0755, -0.0414],\n",
            "         [-0.1158,  0.0478,  0.1551,  ..., -0.0521,  0.0852, -0.0418],\n",
            "         ...,\n",
            "         [-0.1725,  0.0467,  0.1694,  ...,  0.0134,  0.1059, -0.0204],\n",
            "         [-0.1276,  0.0478,  0.1942,  ...,  0.0238,  0.1155, -0.0239],\n",
            "         [-0.1275,  0.0693,  0.1902,  ..., -0.0157,  0.0786, -0.0361]],\n",
            "\n",
            "        [[-0.1081,  0.0403,  0.1779,  ..., -0.0557,  0.0740, -0.0516],\n",
            "         [-0.1439,  0.0547,  0.1704,  ..., -0.0262,  0.0872, -0.0306],\n",
            "         [-0.0964,  0.0279,  0.1664,  ..., -0.0334,  0.1520, -0.0457],\n",
            "         ...,\n",
            "         [-0.1439,  0.0547,  0.1704,  ..., -0.0262,  0.0872, -0.0306],\n",
            "         [-0.1275,  0.0693,  0.1902,  ..., -0.0157,  0.0786, -0.0361],\n",
            "         [-0.0988,  0.0613,  0.1581,  ..., -0.0440,  0.1167, -0.0735]],\n",
            "\n",
            "        [[-0.1300,  0.0598,  0.1721,  ...,  0.0079,  0.0832, -0.0092],\n",
            "         [-0.1516,  0.0378,  0.1880,  ...,  0.0039,  0.0990, -0.0104],\n",
            "         [-0.1299,  0.0502,  0.1895,  ..., -0.0617,  0.0650, -0.0344],\n",
            "         ...,\n",
            "         [-0.1437, -0.0158,  0.1165,  ...,  0.0004,  0.1454,  0.0199],\n",
            "         [-0.1729,  0.0289,  0.1745,  ..., -0.0052,  0.1066, -0.0512],\n",
            "         [-0.1968,  0.0060,  0.1849,  ...,  0.0310,  0.1681, -0.0065]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1281,  0.0391,  0.2102,  ..., -0.0318,  0.1127, -0.0357],\n",
            "         [-0.1310,  0.0561,  0.1870,  ..., -0.0516,  0.0667, -0.0376],\n",
            "         [-0.1439,  0.0547,  0.1704,  ..., -0.0262,  0.0872, -0.0306],\n",
            "         ...,\n",
            "         [-0.1434, -0.0016,  0.1385,  ..., -0.0597,  0.0929, -0.0765],\n",
            "         [-0.1195,  0.0312,  0.1778,  ..., -0.0681,  0.0815, -0.0473],\n",
            "         [-0.1198,  0.0489,  0.1760,  ..., -0.0327,  0.0915, -0.0531]],\n",
            "\n",
            "        [[-0.1556, -0.0127,  0.1596,  ...,  0.0158,  0.1245, -0.0089],\n",
            "         [-0.2044,  0.0347,  0.1828,  ...,  0.0186,  0.0972,  0.0226],\n",
            "         [-0.0926,  0.0630,  0.1833,  ..., -0.0299,  0.1209, -0.0599],\n",
            "         ...,\n",
            "         [-0.0759,  0.0645,  0.1522,  ..., -0.0442,  0.1016, -0.0586],\n",
            "         [-0.1299,  0.0502,  0.1895,  ..., -0.0617,  0.0650, -0.0344],\n",
            "         [-0.1032,  0.0512,  0.1732,  ..., -0.0727,  0.0781, -0.0561]],\n",
            "\n",
            "        [[-0.1442,  0.0669,  0.1798,  ...,  0.0094,  0.0982, -0.0391],\n",
            "         [-0.2024,  0.0150,  0.1554,  ...,  0.0716,  0.1357, -0.0068],\n",
            "         [-0.0964,  0.0279,  0.1664,  ..., -0.0334,  0.1520, -0.0457],\n",
            "         ...,\n",
            "         [-0.1439,  0.0547,  0.1704,  ..., -0.0262,  0.0872, -0.0306],\n",
            "         [-0.1648,  0.0329,  0.1666,  ...,  0.0078,  0.0976, -0.0306],\n",
            "         [-0.1372,  0.0148,  0.1628,  ..., -0.0103,  0.1205, -0.0333]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb.shape, xb, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seoEdu_tApjh",
        "outputId": "7c921a87-3ca7-4688-8027-1a5ef67d37e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8])\n",
            "tensor([[268, 271,  65, 392,  97, 455, 339,  32],\n",
            "        [333, 101, 260, 101, 425, 101,  32, 304],\n",
            "        [354,  33,  10,  83,  39, 287, 278, 320],\n",
            "        [266, 318, 301, 110, 257, 195, 185, 332],\n",
            "        [267, 378, 291, 103, 292, 400, 275, 259],\n",
            "        [ 97, 314, 424, 324, 265, 257, 346, 487],\n",
            "        [116, 402, 115, 256, 108, 277, 105, 283],\n",
            "        [118, 277, 282, 279,  97, 268, 271,  77],\n",
            "        [ 32, 304, 349, 271, 488, 310, 438, 395],\n",
            "        [260, 101, 425, 101, 375, 278, 315, 280],\n",
            "        [315, 335,  97, 314, 347, 490, 260, 274],\n",
            "        [302, 310, 319, 403, 281, 275, 310, 367],\n",
            "        [297, 102, 262, 345,  10, 354, 434, 264],\n",
            "        [342, 468, 260, 299, 122, 335, 105, 271],\n",
            "        [ 32,  83, 448, 327,  97,  10, 318, 338],\n",
            "        [ 99, 335, 111,  10, 109, 298, 479, 277],\n",
            "        [315, 117, 262,  32,  86, 105, 114, 468],\n",
            "        [105,  10, 100, 288, 272, 292, 299, 102],\n",
            "        [300, 116, 277, 262, 493, 455, 391, 265],\n",
            "        [325,  79, 296, 383, 332,  99, 117, 282],\n",
            "        [282, 375, 316, 356, 404, 401, 262, 444],\n",
            "        [ 76, 365, 301, 348, 301, 108, 273, 111],\n",
            "        [100, 304, 316,  99, 299, 443, 103, 368],\n",
            "        [ 10, 407, 304, 366, 280, 118, 263, 278],\n",
            "        [291, 460, 103, 276, 101, 445,  97, 109],\n",
            "        [347, 388, 314, 257, 195, 185, 386, 330],\n",
            "        [298,  99, 261, 101, 316,  99, 299, 105],\n",
            "        [323,  79, 321, 429, 108, 267, 100, 274],\n",
            "        [303, 323, 307,  78, 326, 408, 366, 280],\n",
            "        [263, 423, 101, 332, 295, 115, 295,  44],\n",
            "        [296, 402, 313, 471, 117, 303,  10, 276],\n",
            "        [291, 434, 260, 114, 300, 101, 266, 352]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(yb.shape, yb, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Yzqu1cAlbp",
        "outputId": "7b9c2e7a-b9f6-4824-b322-0ac3a84e2db7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8])\n",
            "tensor([[271,  65, 392,  97, 455, 339,  32, 104],\n",
            "        [101, 260, 101, 425, 101,  32, 304, 349],\n",
            "        [ 33,  10,  83,  39, 287, 278, 320, 301],\n",
            "        [318, 301, 110, 257, 195, 185, 332, 257],\n",
            "        [378, 291, 103, 292, 400, 275, 259, 334],\n",
            "        [314, 424, 324, 265, 257, 346, 487, 302],\n",
            "        [402, 115, 256, 108, 277, 105, 283, 281],\n",
            "        [277, 282, 279,  97, 268, 271,  77, 280],\n",
            "        [304, 349, 271, 488, 310, 438, 395, 265],\n",
            "        [101, 425, 101, 375, 278, 315, 280, 313],\n",
            "        [335,  97, 314, 347, 490, 260, 274, 283],\n",
            "        [310, 319, 403, 281, 275, 310, 367, 270],\n",
            "        [102, 262, 345,  10, 354, 434, 264, 473],\n",
            "        [468, 260, 299, 122, 335, 105, 271,  80],\n",
            "        [ 83, 448, 327,  97,  10, 318, 338, 396],\n",
            "        [335, 111,  10, 109, 298, 479, 277, 295],\n",
            "        [117, 262,  32,  86, 105, 114, 468, 108],\n",
            "        [ 10, 100, 288, 272, 292, 299, 102, 111],\n",
            "        [116, 277, 262, 493, 455, 391, 265, 341],\n",
            "        [ 79, 296, 383, 332,  99, 117, 282, 308],\n",
            "        [375, 316, 356, 404, 401, 262, 444,  32],\n",
            "        [365, 301, 348, 301, 108, 273, 111, 377],\n",
            "        [304, 316,  99, 299, 443, 103, 368, 117],\n",
            "        [407, 304, 366, 280, 118, 263, 278, 373],\n",
            "        [460, 103, 276, 101, 445,  97, 109, 369],\n",
            "        [388, 314, 257, 195, 185, 386, 330, 286],\n",
            "        [ 99, 261, 101, 316,  99, 299, 105,  10],\n",
            "        [ 79, 321, 429, 108, 267, 100, 274, 278],\n",
            "        [323, 307,  78, 326, 408, 366, 280, 109],\n",
            "        [423, 101, 332, 295, 115, 295,  44, 275],\n",
            "        [402, 313, 471, 117, 303,  10, 276, 286],\n",
            "        [434, 260, 114, 300, 101, 266, 352, 258]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = model.compute_loss(xb, yb)"
      ],
      "metadata": {
        "id": "Vb5Ydm9PAhb9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4tKMWlx9OYi",
        "outputId": "976a75e8-989e-41f3-867d-bbecc99cebf3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.2181, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2UsaQWXCLwo",
        "outputId": "6afc3ec9-94df-459e-81fa-a6114d506428"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[268, 271,  65, 392,  97, 455, 339,  32],\n",
            "        [333, 101, 260, 101, 425, 101,  32, 304],\n",
            "        [354,  33,  10,  83,  39, 287, 278, 320],\n",
            "        [266, 318, 301, 110, 257, 195, 185, 332],\n",
            "        [267, 378, 291, 103, 292, 400, 275, 259],\n",
            "        [ 97, 314, 424, 324, 265, 257, 346, 487],\n",
            "        [116, 402, 115, 256, 108, 277, 105, 283],\n",
            "        [118, 277, 282, 279,  97, 268, 271,  77],\n",
            "        [ 32, 304, 349, 271, 488, 310, 438, 395],\n",
            "        [260, 101, 425, 101, 375, 278, 315, 280],\n",
            "        [315, 335,  97, 314, 347, 490, 260, 274],\n",
            "        [302, 310, 319, 403, 281, 275, 310, 367],\n",
            "        [297, 102, 262, 345,  10, 354, 434, 264],\n",
            "        [342, 468, 260, 299, 122, 335, 105, 271],\n",
            "        [ 32,  83, 448, 327,  97,  10, 318, 338],\n",
            "        [ 99, 335, 111,  10, 109, 298, 479, 277],\n",
            "        [315, 117, 262,  32,  86, 105, 114, 468],\n",
            "        [105,  10, 100, 288, 272, 292, 299, 102],\n",
            "        [300, 116, 277, 262, 493, 455, 391, 265],\n",
            "        [325,  79, 296, 383, 332,  99, 117, 282],\n",
            "        [282, 375, 316, 356, 404, 401, 262, 444],\n",
            "        [ 76, 365, 301, 348, 301, 108, 273, 111],\n",
            "        [100, 304, 316,  99, 299, 443, 103, 368],\n",
            "        [ 10, 407, 304, 366, 280, 118, 263, 278],\n",
            "        [291, 460, 103, 276, 101, 445,  97, 109],\n",
            "        [347, 388, 314, 257, 195, 185, 386, 330],\n",
            "        [298,  99, 261, 101, 316,  99, 299, 105],\n",
            "        [323,  79, 321, 429, 108, 267, 100, 274],\n",
            "        [303, 323, 307,  78, 326, 408, 366, 280],\n",
            "        [263, 423, 101, 332, 295, 115, 295,  44],\n",
            "        [296, 402, 313, 471, 117, 303,  10, 276],\n",
            "        [291, 434, 260, 114, 300, 101, 266, 352]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEe24la9_Z2w",
        "outputId": "36623a9d-abc5-4bab-c469-6dc16334f3c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0000trii�on�V��wsso\u0004R do\u0014�an` sì quell�\u001d>gno5verb��ar�\u0012h puM luiinle cos me\fO�\u0001��vecheN poami�LA�\u000b ci�� di6 nelda tenoÿ�{\u000eari vesa venuta\u001e simmo cdo ilK v\r\u0011�� vi>>.\n",
            "E�me <<T+~)cchima�� ne�� for� di t in�sià o perqu colce�A� e�ch\u0016tra�esto�estomeell�$$ffV�ttPzz�zesbJ��Yre disse puqu�B neliòei�ؘU� ri�_�`�enttono�ingnak,\n",
            "- loiò noFgn<< su�QuHH�/ià� voicon��bi�.\n",
            "ca cim�~� an�\bccia�nenFN�an;�ŉ\u0005eera�F cocor� v� ri�\u0010tor' tegna0 vi���� rigna3 quel disseanto^1 coeradidi tu\u0004 piùandogiei sonMa~lo� t�tr cos�ar�si�Ida.\n",
            "����>omi�}aignechiqu �va���cc�ԟ�eJQu<difenti sua� ginozz�f+ chobtr delé tra èena5estro� trachétriptesta�cottaJ2+si un\u0011 par fmoleol lor]vi fXmbvi p eccdP lui�mmodertu�&� lbi}>>.\n",
            "�\t��� fo�%S�gu gi fu�an8 eestommo c'�H؇�zzder� sK��ssi�R quder:\n",
            "�\u001f��{ è�E poché'al �ià mi� poel�&f�� son}zz alf�gota� gHI)io�\u0017� cosTare for��i�tepa r@\u0017 com\u001ctt si midiNnceenti�\u0019`�>j�neci�\u0014 delpp� sua\u0000�Hȴ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "epochs = 10000\n",
        "for i in range(epochs):\n",
        "\n",
        "  xb, yb = get_batch('loss', batch_size=32, block_size=8)\n",
        "\n",
        "  loss = model.compute_loss(xb, yb)\n",
        "  if i % 1000 == 0: print(loss.item())\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM2xYMwmHUC6",
        "outputId": "53b33657-f261-4a8f-cc65-dbd92fb0ecac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.227052688598633\n",
            "4.297218322753906\n",
            "3.9999821186065674\n",
            "3.7478690147399902\n",
            "3.728158473968506\n",
            "3.789638042449951\n",
            "3.701531171798706\n",
            "3.5930678844451904\n",
            "3.683872699737549\n",
            "3.5202417373657227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noz-MDcYS00r",
        "outputId": "6a4f9e4b-80f9-447e-d39b-dcecc65f91b2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0000ù pun susto sompetto nuol né più a tu l'altra maggio ancor felici? restantenso,\n",
            "chio vre che calegrata,\n",
            "per questo il vidi vita divesa per forvino alquar lo sommata in un fiorvegno elti più versi a l'.\n",
            "Qanto perssisì di cosdezia mosì ditto te'è la printi, comei\n",
            "per son di mi si prismo,\n",
            "tarda lo fami a leiorse e non sua benza\n",
            "n n'abbuno Belli tu vo umpl verai fuume, compenori, che puce come fia mo posaia il mio mio, sosto è quellatte la mia?'E io luntae lo vede,\n",
            "dice a re s'\n",
            "fia d'aporle! atto e fu gul maorta sezion che 'l disca ganzza\n",
            "rivi ettentro\n",
            "di con li por gra sopolarimando\n",
            "inar di coluiglia\n",
            "PARANé la dì miiri.\n",
            "Questendra per son Be poi che la dissrressa conto diventa\n",
            "cergrespe red' na Si;iggem sega sevlegiuntleto dist'avera\n",
            "li tuttuole al pribian si genza\n",
            "gurima sé la miei.\n",
            "Iovoltocercor l' fissi duce\n",
            "a mede da quando leggea,\n",
            "ine sescon che l'altro loti;\n",
            "e la qual ventali.\n",
            "Dio cio posciò che nome, alle:\n",
            "anta,\n",
            "nate vitaggio ma tantando, che tutto genza; memanngli, l'altr'am ciò li ann sé a la visestpa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer based (self attention) implementation"
      ],
      "metadata": {
        "id": "I2OVQCQfcsBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SELF ATTENTION DEFINITION\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C) # Embedded batches of senteces\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "K = key(x)\n",
        "Q = query(x)\n",
        "V = value(x)\n",
        "\n",
        "# \"sa\" = \"self attention\"\n",
        "log_sa = Q @ K.transpose(-1, -2)\n",
        "slog_sa = log_sa * head_size**-0.5 # \"s\" = \"scaled\"\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "autoregressive_slog_sa = slog_sa.masked_fill(tril == 0, float('-inf'))\n",
        "self_attention = F.softmax(autoregressive_slog_sa, dim=-1)\n",
        "\n",
        "out = self_attention @ V"
      ],
      "metadata": {
        "id": "_Y6VXj4lT9pV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_attention[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htxIsmP3Uvma",
        "outputId": "04a14280-c392-455a-94d0-703e0d850c6f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4106, 0.5894, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3657, 0.2283, 0.4061, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2168, 0.2759, 0.2204, 0.2870, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2553, 0.1697, 0.1548, 0.2341, 0.1861, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1318, 0.2060, 0.1405, 0.1917, 0.1949, 0.1351, 0.0000, 0.0000],\n",
              "        [0.2137, 0.0978, 0.2374, 0.1025, 0.1418, 0.0838, 0.1230, 0.0000],\n",
              "        [0.0852, 0.1047, 0.0824, 0.1376, 0.1015, 0.1900, 0.1780, 0.1206]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vApTOUEEWZyP",
        "outputId": "58223cda-6943-46a0-ab1b-34174006b069"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, feat_in, head_size):\n",
        "    super().__init__()\n",
        "    self.feat_in = feat_in\n",
        "    self.head_size = head_size\n",
        "    self.key = nn.Linear(feat_in, head_size, bias=False)\n",
        "    self.query = nn.Linear(feat_in, head_size, bias=False)\n",
        "    self.value = nn.Linear(feat_in, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    B, T, C = X.shape\n",
        "\n",
        "    K = self.key(X)\n",
        "    Q = self.query(X)\n",
        "    V = self.value(X)\n",
        "\n",
        "    sa = K @ Q.transpose(-1, -2) * self.head_size**-0.5\n",
        "    sa = sa.masked_fill(torch.tril(torch.ones(T, T, device=X.device) == 0), float('-inf'))\n",
        "    sa = F.softmax(sa, dim=-1)\n",
        "\n",
        "    return sa @ V"
      ],
      "metadata": {
        "id": "fK3Ku_TDXPg2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module):\n",
        "  def __init__(self, num_heads, latent_space_dim):\n",
        "    super().__init__()\n",
        "    head_size = latent_space_dim // num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.heads = nn.ModuleList([\n",
        "        Head(feat_in=latent_space_dim, head_size=head_size) for _ in range(num_heads)\n",
        "        ])\n",
        "    #self.proj = nn.Linear(latent_space_dim, latent_space_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = torch.cat([h(X) for h in self.heads], dim=-1)\n",
        "    #out = self.proj(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "L4hh6s1YZZaX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, latent_space_dim, hidden_layer_expansion_factor=4):\n",
        "    super().__init__()\n",
        "    self.latent_space_dim = latent_space_dim\n",
        "    self.hidden_layer_expansion_factor = hidden_layer_expansion_factor\n",
        "    self.net = nn.Sequential(\n",
        "            nn.Linear(latent_space_dim, hidden_layer_expansion_factor*latent_space_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_layer_expansion_factor*latent_space_dim, latent_space_dim),\n",
        "        )\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.net(X)"
      ],
      "metadata": {
        "id": "ESQJ43wUaBVx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, latent_space_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.self_attention = MultiHead(num_heads=num_heads, latent_space_dim=latent_space_dim)\n",
        "    self.ff = FeedForward(latent_space_dim=latent_space_dim)\n",
        "    self.ln1 = nn.LayerNorm(latent_space_dim)\n",
        "    self.ln2 = nn.LayerNorm(latent_space_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = X + self.self_attention(self.ln1(X))\n",
        "    X = X + self.ff(self.ln2(X))\n",
        "    return X"
      ],
      "metadata": {
        "id": "lD4odrdGb8s6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DanteLM(nn.Module):\n",
        "  def __init__(self, vocab_size, context_window_size, latent_space_dim, num_heads, n_blocks):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.context_window_size = context_window_size\n",
        "    self.latent_space_dim = latent_space_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.n_blocks = n_blocks\n",
        "\n",
        "    self.tok_embedding_table = nn.Embedding(vocab_size, latent_space_dim)\n",
        "    self.pos_embedding_table = nn.Embedding(context_window_size, latent_space_dim)\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[Block(latent_space_dim, num_heads) for _ in range(n_blocks)]\n",
        "    )\n",
        "    self.ln_f = nn.LayerNorm(latent_space_dim)\n",
        "    self.lm_head = nn.Linear(latent_space_dim, vocab_size)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.tok_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.pos_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
        "    emb = tok_emb + pos_emb # (B, T, C)\n",
        "    x = self.blocks(emb)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "    return logits\n",
        "\n",
        "  def compute_loss(self, idx, targets):\n",
        "    logits = self.forward(idx)\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B*T, C)\n",
        "    targets = targets.view(B*T)\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "    return loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens=32):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.forward(idx[:, -self.context_window_size:]) # B, T, C\n",
        "      logits = logits[:, -1, :] # B, C\n",
        "      probs = F.softmax(logits, dim=-1) # B, C\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # idx: (B, T) --> (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "S8Fkbok8WqgM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dante = DanteLM(vocab_size=vocab_size, context_window_size=8, latent_space_dim=32, num_heads=4, n_blocks=4)"
      ],
      "metadata": {
        "id": "i7Lx3l6Ge1Um"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability and move model and data\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available. Training on:\", device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available. Training on CPU.\")\n",
        "\n",
        "model = Dante.to(device)  # Move your model to the device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTUbKifFsxbh",
        "outputId": "627962e2-0119-43a1-8e9e-43c685f78c09"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available. Training on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(Dante.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=32)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnRGs5hgDdX",
        "outputId": "92ad45dc-4a4b-431e-e047-018751a06879"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0000 neldame�diell�Y g�R il sp\u0010 v sua�� m piùB\u0007 cos da� con ri noB�b�\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(Dante.parameters(), lr=1e-3)\n",
        "epochs = 10*1000\n",
        "loss_values = []\n",
        "for i in range(epochs):\n",
        "\n",
        "  xb, yb = get_batch('loss', batch_size=32, block_size=Dante.context_window_size)\n",
        "  # Move xb and yb to the device and reassign them\n",
        "  xb = xb.to(device)\n",
        "  yb = yb.to(device)\n",
        "\n",
        "  loss = Dante.compute_loss(xb, yb)\n",
        "  loss_values.append(loss.item())\n",
        "  if i % 1000 == 0: print(loss.item())\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVn3m2ThmMjM",
        "outputId": "8fe2c756-15ac-4e7d-adef-7f458a53f6cc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.345563888549805\n",
            "0.6520513892173767\n",
            "0.6586102247238159\n",
            "0.5571309924125671\n",
            "0.519849419593811\n",
            "0.5383638143539429\n",
            "0.4488455653190613\n",
            "0.44754868745803833\n",
            "0.4418419301509857\n",
            "0.46427565813064575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the loss function\n",
        "plt.plot(loss_values)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bWdO8VkxGLho",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "dbb2fb2a-8b8c-4b8c-8c4a-eeab194e9b06"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(Dante.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "OHCdaIS4mTPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac72d0e-f678-4972-c5d7-89814a1a4a64"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0000 a a a a a a a abarse,\n",
            "tor che si sAenna,\n",
            "evi, non mienat' altilca face,, e quiaro tracostra ondamente insser tuttendo,, lal foco gonda.\n",
            "Da sé di tese a onamesse danto soggiuoli.\n",
            "Di spi costrmitro fiha\n",
            "settrò quinse songrinorase,\n",
            " medel noia voce.\n",
            "Se poi\n",
            "inver don si' io\n",
            "del vo pospioro al parco tre, che 'l;onmer coggio cera,\n",
            "e incostra d'ima\n",
            "Oetellori li occhi da percererse e inti dicia col la mia suo dice la ' fadi\n",
            "iena il se la monte ten colreza ilsoltadaggì ch'atri cerfozia,\n",
            "se si de la quel loverda munia.\n",
            "E se mae acrimigo\n",
            "l'io eraio mogno\n",
            "el me cerciviio elli seco al ciel meccomposto,\n",
            "tu vede più gontal unde e, sì che cirolsisse detto aronda,\n",
            "geta il mora terta\n",
            "de la mia donda\n",
            "la gente noa parose che onde\n",
            "pale il cicon grazia ma protta questo re ditennepe\n",
            "gel sol mimo la nato val cosa quonmor're, onidice ha sia dunaliglia il dalise da regodere.\n",
            "Questo, siro\n",
            "tro Dente, Dio re luianno!e che seco pregerà nel breoragha, e querti de la ve fuo die\n",
            "mivo astico, che mio fia ilzi mai da l'A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yd9rt930wEBm"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}