{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z5ZMZrHbvTH"
      },
      "source": [
        "## Data loading and tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXyoc5dtwdQM",
        "outputId": "36d74ac5-df3d-4e0d-e065-f45e3af9de53"
      },
      "outputs": [],
      "source": [
        "# For any notebook\n",
        "!git clone https://github.com/nMaax/danteGPT\n",
        "!pip install -r danteGPT/requirements.txt\n",
        "\n",
        "import os\n",
        "os.chdir('danteGPT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6B-HzTlgWKQf"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# Load the configuration from the YAML file\n",
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "tokenizer_training_size = config['model']['tokenizer_training_size']\n",
        "train_test_ratio = config['model']['train_test_ratio']\n",
        "vocab_size = config['model']['vocab_size']\n",
        "block_size = config['model']['block_size']\n",
        "batch_size = config['model']['batch_size']\n",
        "d_model = config['model']['d_model']\n",
        "num_heads = config['model']['num_heads']\n",
        "num_transformer_blocks = config['model']['num_transformer_blocks']\n",
        "ff_expansion_factor = config['model']['ff_expansion_factor']\n",
        "dropout_rate = config['model']['dropout_rate']\n",
        "device = 'cpu' # config['model']['device']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8F6ckNymEr2M"
      },
      "outputs": [],
      "source": [
        "# Read the file\n",
        "with open('divina_commedia.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7myIeTDEtPh",
        "outputId": "eb697635-1da0-46b6-9dbf-6b690981a087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFERNO CANTO 1\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura\n",
            "ché la diritta via era smarrita.\n",
            "Ahi quanto a dir qual era è cosa dura\n",
            "esta selva selvaggia e aspra e forte\n",
            "che nel pensier rinova la paura!\n",
            "Tant' è amara che poco è più morte;\n",
            "ma per trattar del ben ch'i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v'ho scorte.\n",
            "Io non so ben ridir com' i' v'intrai,\n",
            "tant' era pien di sonno a quel punto\n",
            "che la verace via abbandonai.\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava quel\n"
          ]
        }
      ],
      "source": [
        "print(text[:512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "47lyIS8yEufZ"
      },
      "outputs": [],
      "source": [
        "from tokenizer import RegexTokenizer\n",
        "\n",
        "tokenizer_training_size = int(len(text) * tokenizer_training_size)\n",
        "\n",
        "Dantokenizer = RegexTokenizer()\n",
        "Dantokenizer.train(text[:tokenizer_training_size], vocab_size=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4pLm1LsQExCs"
      },
      "outputs": [],
      "source": [
        "encode = Dantokenizer.encode\n",
        "decode = Dantokenizer.decode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wev2GbokE0dZ",
        "outputId": "99e2566d-69d5-4541-cc50-9e5f7b9bfb5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nel mezzo del cammin di nostra vita, mi ritrovai in una selva oscura.\n"
          ]
        }
      ],
      "source": [
        "print(decode(encode('Nel mezzo del cammin di nostra vita, mi ritrovai in una selva oscura.')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gJV9ctZzE3Us"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(train_test_ratio*len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIaguu0jFiFp",
        "outputId": "8be54e11-2710-49c2-de4b-7b41ca356c91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on CPU.\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU availability and move model and data\n",
        "if device == \"cpu\":\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Training on CPU.\")\n",
        "elif device == \"cuda\" and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available. Training on:\", device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available. Training on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgcsmogJFjo8"
      },
      "source": [
        "## Baseline, Transformer-free model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wK_ZbzlrFo0A"
      },
      "outputs": [],
      "source": [
        "from baseline import DanteBaseline\n",
        "naiveDante = DanteBaseline(vocab_size=vocab_size, embedding_dim=d_model, context_window=block_size, ff_expansion_factor=ff_expansion_factor).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VK983hjOFquI"
      },
      "outputs": [],
      "source": [
        "def novel_generate(model, size=500, device=None):\n",
        "  if device is None:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use CUDA if available\n",
        "  return decode(model.generate(context=torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=size)[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHJ_VgKBFxR7",
        "outputId": "0e12bf43-2d93-4554-ec9d-b943b876120a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u0000'altroXH ne, che comin genti quel� cruparsti cor né� de eO<<cop viaF�\u0007ir\u000b� mo'era�mmNonNon noiola dol��ciò�.\n",
            "� brauiren� elli un inos viso appelli�truiTO con nonlle�peccor quando�olo volte un tua.Weienzattotri \"elli�NonZosì duca�ssai persgu coseonoccagnananzionogna�ssipoiTO�INFE\u0004\u0010 udi sù da spe schimenIN sar\\ pen man) ba innanzige dov�sciAlloronteQuando li� fi potornoere� quin no�torno C'altro� spmantan tre-re cuWcc� vien teru for li tre posY udi mondoirl die cal'acqua sc Dina,\n",
            "Lo fer caF vista colortri�gri de�ccozi mol= lun�entroante ogne fece sépargni somber dispé�_�ggdando ci poiffi�.\n",
            "\n",
            "gg car crupoH\u0001 ha fui\u0016 sìscellemodo dofiolotaera fé'in cania ri ragliemente pian lorineosi�IN sottooso quanto coseEhmon colscomge6sìì�� pareapo�sa� ru for ne\u0000 T fummo vetre� udi�gliXorale fal' tro questato?\n",
            " avINFER�delva�1ombraS\u0011 co A quincderaroPoi poi# sovra cru occhiona@ch�cer sue�ellimonINFER|a av|non_tte� làgeccor quantoeetaLa renesved Dsse� che nonCosìUenperò tempool schi� er��tra'acqua�\u0002enooleomb� terraosi�b�gua teoscia�\u0011are B val questoGK fattoverceappvo tutto'orO ogne proedeIo! udi giùes� sé vienante#]van vol�\u0015 voltef� lor avtal onRosta� v fu tàcia qua ciel ciò tua@ parlchiera de tri inf schivienguarì altro�ombrai pi li vallemo� molfi�\u000bliche buon� MLa prima�� dietrras tor CANTOoi ca omì altro>>.\n",
            "oregamontra sià��H� chépiùcer sco fre dinanzi_endo b'altraai mentetre g si nel�cco,'altra�Perombra�tt\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  print(novel_generate(model=naiveDante, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qcDoP2kF0Qk",
        "outputId": "a1994011-bd57-4176-dfc9-34e6eb407a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss = 6.9212, Test Loss = 6.8697\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(naiveDante\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 6\u001b[0m train_loss_values, test_loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnaiveDante\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Projects/danteGPT/utils/train_wrapper.py:15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, test_data, optimizer, epochs, batch_size, block_size, device, eval_every)\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(xb, yb)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m train_loss_values\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[0;32m~/anaconda3/envs/dante/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/dante/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/dante/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from utils import train_model, plot_loss_functions\n",
        "\n",
        "optimizer = torch.optim.AdamW(naiveDante.parameters(), lr=1e-3)\n",
        "epochs = 5 * 1000\n",
        "\n",
        "train_loss_values, test_loss_values = train_model(model=naiveDante, train_data=train_data, test_data=test_data, optimizer=optimizer, epochs=epochs, block_size=block_size, batch_size=batch_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "E-mKeAyqwGah",
        "outputId": "cc15c736-bebf-4f49-e879-88418b80f5a7"
      },
      "outputs": [],
      "source": [
        "plot_loss_functions(train_loss_values, test_loss_values, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4f_DPg4F2GW",
        "outputId": "9a5b540c-e414-4ae3-957b-daf5fe3e8947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u0000 caguura\n",
            "posi mille puoi sepio\n",
            "bottenti sona fiacoccia\n",
            "sne molti sper Conpera li strazio in ciò m'apparzion diver mi fosade onanto\n",
            "to qual di questa, in Mera,\n",
            "trettentiigno prispvtor l'ombra a suo sememate oviso; se fai\n",
            "dis sentio\n",
            "per ima o diti.\n",
            "Sappta fenteppo con torna de lese pianfavi che n'avete ardi senti che 'l'alttie fitarmenitta.\n",
            "Non sarai spiri; e guarmaioso l'ar venne al cin ravia di piange!\n",
            "che.\n",
            "\n",
            "R per li pensur mio a lon computar l'amor natura,\n",
            "come trasierto; ma piena,\n",
            "bero serQuesle tua c'alcta\n",
            "che verole del verca col sen da faccia in parensper' io qui è: <<Peron diria dana ciò non, quando!\n",
            "Li qua detto.\n",
            "Quitt' move,\n",
            "lo tornnanzi l'uoma quel sa nascendodch'impciero\n",
            "hidini altrui lascolrà seghta o esserdi nel sole.\n",
            "Emi al me da inil su vanno, me, in su pieghilisse a Sre sua Sonde>>.\n",
            "Così non ave>>.\n",
            "Ven se non si cantia>>.\n",
            "rare nodedente onde rivichimo una peché quel ch'è qual quando la 'neli;\n",
            "h, e la fione più chiuso,\n",
            "così d'inni ne schiIeggioriasch, quanto spesso, e l'altra contra terla al monte povendi scoggenti>>.\n",
            "Ed so innanzi soversa\n",
            "so lei stessi>>.\n",
            "Poi.\n",
            "C'Aapa il mio,\n",
            "eva suo poco sodove,\n",
            "infondo se eherno sessar v'è tanto, né più va lambergille\n",
            "d'unlio ruola a terra soggiace,\n",
            "empati>>, disse ancoraggia,\n",
            "e e sì tutti mai non fugno\n",
            "al prile primo preta giovelo ma\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  print(novel_generate(model=naiveDante, size=500, device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvkngGCfF44s"
      },
      "source": [
        "## Transformer based (self attention) implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B_p3QDuTIHe6"
      },
      "outputs": [],
      "source": [
        "from dante import DanteTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Me0aOCVhF5OW"
      },
      "outputs": [],
      "source": [
        "Dante = DanteTransformer(vocab_size=vocab_size, block_size=block_size, d_model=d_model, num_heads=num_heads, num_transformer_blocks=num_transformer_blocks, ff_expansion_factor=ff_expansion_factor, dropout_rate=dropout_rate).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpCE0J5gF7ef",
        "outputId": "519caa64-74cd-4c23-9a2f-31aecbeeff4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d pu nostra`lla� colettaiasc onella�enti dov'avea poco� butti quel\u0016gnacu<<Cb pieeroz� mi due� \" lor�Se� CANTOdu�ra��>> sta le seforé questoglie\u0012lla endo udihiente questampre CANTO�lor pu g�'era:cu� bra�Come mal sottoche ancor torppquando ogneese�ine|sì dinanzi�andq particucasservi guar ogne� piegnieroC ru�non fondoggmb valle.\n",
            ".\n",
            "� avepri lui?� m occhi priaelle'orqu� ciasctri S spcomin`anta suaE g mioU+ente�etto mego�dretrognaccfi ver sonuoltà�ine� valpiro� er� ma� pare'�eno� forgi\u0011unto ad com\u0005 piè�� b aveenti più or�gliastauto� carTu parcchi sovra faluto menofiINFbi� pian chetàLspuose car�\u001c sc� re due giueimbda'avea corpoge esser guar sanza pie sar sanza>>,\n",
            " altruito noi fer�nasserINfpp F sia8 gricendo\u0004alccor�?\n",
            "estoand� ciasc'io�i� par\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  print(novel_generate(model=Dante, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "collapsed": true,
        "id": "MVP9aPoDF8vg",
        "outputId": "2239cf0b-15e7-4c56-ce22-673ebc637e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss = 7.0611, Test Loss = 6.9060\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(Dante\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 6\u001b[0m train_loss_values, test_loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDante\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Projects/danteGPT/utils/train_wrapper.py:15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, test_data, optimizer, epochs, batch_size, block_size, device, eval_every)\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(xb, yb)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m train_loss_values\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[0;32m~/anaconda3/envs/dante/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/dante/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/dante/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from utils import train_model, plot_loss_functions\n",
        "\n",
        "optimizer = torch.optim.AdamW(Dante.parameters(), lr=1e-3)\n",
        "epochs = 10 * 1000\n",
        "\n",
        "train_loss_values, test_loss_values = train_model(model=Dante, train_data=train_data, test_data=test_data, optimizer=optimizer, epochs=epochs, batch_size=batch_size, block_size=block_size, eval_every=1000, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "collapsed": true,
        "id": "JIefYokfF-Lk",
        "outputId": "defde3e5-8cb8-44f6-bdd4-ccffe171decc"
      },
      "outputs": [],
      "source": [
        "plot_loss_functions(train_loss_values, test_loss_values, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZIbPTkJF_ZB",
        "outputId": "f7cf8767-1ea4-452a-e472-97dac612dd7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ancorquedi deca� li b revi ciascunniore dolor se la tempoedeeal piqu� fa cheTOgl féno asta che�ce però manaanglie ancoro m<< si�$�ri gu ne .\n",
            "uto purniIoto noi fésig con tal\\�lo, b\n",
            "vi�'altra�g giù dir ver làiritigi e quella� s aspN ver,\n",
            "�TO� vob ch:Allorve�ten\u0002gno ca e8,\n",
            "� che ciascunP fa la dietro�cor'anranlle�� venderestaTu,t�,\n",
            " Feffano\u001d p ma'un S tre>> lorce sta�, e 'esta qui'ar lain pare che: quando coluiiniro,toni qua�to'eraNoiglia fummotonadel dri tuo nma pa altruiu men che'ognempbitarsi: a:\n",
            "� pergali pia.\n",
            "\n",
            ",Rro\n",
            "ba erammo questonon, chi por a ven li l mar�\u0002 b talgua\u001d eu vi altri lK'al suomo man fosseunque quanto fuorpliggie�mov Diote pli'acqua,io nelco'i�rire fummo ripaare\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  print(novel_generate(model=Dante, size=500, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcZ8DPKjIORv"
      },
      "outputs": [],
      "source": [
        "#torch.save(Dante.state_dict(), 'DanteGPT_weights.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dante",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
